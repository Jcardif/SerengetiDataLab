{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "serengetidatalabqezfaqkj"
		},
		"serengetidatalabqezfaqkj-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'serengetidatalabqezfaqkj-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:serengetidatalabqezfaqkj.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"VaultSerengeti_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://serengetikeyvaultqezfaqk.vault.azure.net/"
		},
		"serengetidatalabqezfaqkj-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://serengetistoreqezfaqkji2.dfs.core.windows.net/"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/MainPipeline')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "copy_zipped_metadata_files",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "copy_zipped_metadata_files",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"numExecutors": null
						}
					},
					{
						"name": "save_json_data_to_sql",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "copy_zipped_metadata_files",
								"dependencyConditions": [
									"Completed"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "save_json_data_to_sql",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"numExecutors": null
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/copy_zipped_metadata_files')]",
				"[concat(variables('workspaceId'), '/notebooks/save_json_data_to_sql')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/VaultSerengeti')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('VaultSerengeti_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/serengetidatalabqezfaqkj-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('serengetidatalabqezfaqkj-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/serengetidatalabqezfaqkj-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('serengetidatalabqezfaqkj-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/drop_tables')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "DROP TABLE val\nDROP TABLE train\nDROP TABLE images\nDROP TABLE categories\nDROP TABLE annotations\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "defdedicated",
						"poolName": "defdedicated"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/select')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT * FROM train\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "defdedicated",
						"poolName": "defdedicated"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/copy_zipped_metadata_files')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "defsparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "224g",
					"driverCores": 32,
					"executorMemory": "224g",
					"executorCores": 32,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5b1d34e4-f7bb-4610-9502-af6e2b8c2f05"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "csharp"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a1a27566-3e3c-42d7-a372-692095cd8521/resourceGroups/SerengetiDataLab/providers/Microsoft.Synapse/workspaces/serengetidatalabqezfaqkj/bigDataPools/defsparkpool",
						"name": "defsparkpool",
						"type": "Spark",
						"endpoint": "https://serengetidatalabqezfaqkj.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/defsparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 32,
						"memory": 224
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#r \"nuget: Microsoft.Azure.Storage.Common\"\n",
							"#r \"nuget: Microsoft.Azure.Storage.Blob\"\n",
							"#r \"nuget: Microsoft.Azure.Storage.File\""
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"using Microsoft.Azure.Storage;\n",
							"using Microsoft.Azure.Storage.Blob;\n",
							"using Microsoft.Azure.Storage.File;\n",
							"using System.IO;\n",
							"using System.IO.Compression;\n",
							"\n",
							"using CopyStatus = Microsoft.Azure.Storage.Blob.CopyStatus;\n",
							"using Microsoft.Spark.Extensions.Azure.Synapse.Analytics.Utils;\n",
							"using Microsoft.Azure.Storage.Auth;\n",
							"using Microsoft.Spark.Extensions.Azure.Synapse.Analytics.Notebook.MSSparkUtils;"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"CloudBlobContainer destContainer, sourceContainer;\n",
							"CloudBlobDirectory destDirectory;\n",
							"\n",
							"var vaultName = \"serengetikeyvaultqezfaqk\";\n",
							"\n",
							"string destConnectionString = Credentials.GetSecret(vaultName,\"ADLS-ConnectionString\");"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"private async Task InitSourceAndDest ()\n",
							"{\n",
							"    // Create a BlobEndpoint for the source container\n",
							"    string sourceConnectionString = \"BlobEndpoint=https://lilablobssc.blob.core.windows.net;\";\n",
							"    CloudStorageAccount sourceStorageAccount = CloudStorageAccount.Parse(sourceConnectionString);\n",
							"    CloudBlobClient sourceBlobClient = sourceStorageAccount.CreateCloudBlobClient();\n",
							"    sourceContainer = sourceBlobClient.GetContainerReference(\"snapshotserengeti-v-2-0\");\n",
							"\n",
							"    // Create a FileEndpoint for the destination ADLS\n",
							"    CloudStorageAccount destStorageAccount = CloudStorageAccount.Parse(destConnectionString);\n",
							"\n",
							"    var destBlobClient= destStorageAccount.CreateCloudBlobClient();\n",
							"    destContainer =  destBlobClient.GetContainerReference(\"snapshot-serengeti\");\n",
							"\n",
							"    if(! await destContainer.ExistsAsync())\n",
							"    {\n",
							"        await destContainer.CreateIfNotExistsAsync();\n",
							"    }\n",
							"\n",
							"    destDirectory = destContainer.GetDirectoryReference(\"metadata\");\n",
							"}"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"private async Task ExtractAndSaveZippedBlob(CloudBlockBlob sourceBlob,   CloudBlobDirectory destDirectory)\n",
							"{\n",
							"    using (var sourceStream = sourceBlob.OpenRead())\n",
							"    using (var archive = new ZipArchive(sourceStream))\n",
							"    {\n",
							"        foreach (var entry in archive.Entries)\n",
							"        {\n",
							"            var destBlob = destDirectory.GetBlockBlobReference(entry.FullName);\n",
							"\n",
							"            if (await destBlob.ExistsAsync())\n",
							"            {\n",
							"                Console.WriteLine($\"{destBlob.Name} exists\");\n",
							"                continue;\n",
							"            }\n",
							"\n",
							"\n",
							"            using (var entryStream = entry.Open())\n",
							"            {\n",
							"                Console.WriteLine($\"Uploading {destBlob.Name}\");\n",
							"                await destBlob.UploadFromStreamAsync(entryStream);\n",
							"            }\n",
							"        }\n",
							"    }\n",
							"}"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"private async Task CopyBlob(CloudBlockBlob sourceBlob, CloudBlockBlob destBlob)\n",
							"{\n",
							"    if (await destBlob.ExistsAsync())\n",
							"    {\n",
							"        Console.WriteLine($\"{destBlob.Name} exists\");\n",
							"        return;\n",
							"    }\n",
							"\n",
							"    await destBlob.StartCopyAsync(sourceBlob.Uri);\n",
							"\n",
							"    while (destBlob.CopyState.Status == CopyStatus.Pending)\n",
							"    {\n",
							"        await Task.Delay(1000); //wait for 1 second before checking again\n",
							"        await destBlob.FetchAttributesAsync();\n",
							"    }\n",
							"\n",
							"    if (destBlob.CopyState.Status == CopyStatus.Success)\n",
							"    {\n",
							"        Console.WriteLine(\"Copy operation succeeded\");\n",
							"    }\n",
							"    else\n",
							"    {\n",
							"        Console.WriteLine(\"Copy operation failed\");\n",
							"    }\n",
							"}\n",
							""
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"// s01 to s011\n",
							"var metadataZippedFiles=new string[]\n",
							"{\n",
							"    \"SnapshotSerengetiS01.json.zip\",\n",
							"    \"SnapshotSerengetiS02.json.zip\",\n",
							"    \"SnapshotSerengetiS03.json.zip\",\n",
							"    \"SnapshotSerengetiS04.json.zip\",\n",
							"    \"SnapshotSerengetiS05.json.zip\",\n",
							"    \"SnapshotSerengetiS06.json.zip\",\n",
							"    \"SnapshotSerengetiS07.json.zip\",\n",
							"    \"SnapshotSerengetiS08.json.zip\",\n",
							"    \"SnapshotSerengetiS09.json.zip\",\n",
							"    \"SnapshotSerengetiS10.json.zip\",\n",
							"    \"SnapshotSerengetiS11.json.zip\",\n",
							"    \"SnapshotSerengetiBboxes_20190903.json.zip\",\n",
							"\n",
							"};\n",
							"\n",
							"await InitSourceAndDest();\n",
							"\n",
							"foreach (var zippedFile in metadataZippedFiles)\n",
							"{\n",
							"    var sourceBlob = sourceContainer.GetBlockBlobReference(zippedFile);\n",
							"    await ExtractAndSaveZippedBlob(sourceBlob, destDirectory);\n",
							"}\n",
							"\n",
							"var jsonFiles = new string[]\n",
							"{\n",
							"        \"SnapshotSerengetiSplits_v0.json\"\n",
							"};\n",
							"\n",
							"foreach (var file in jsonFiles)\n",
							"{\n",
							"    var sourceBlob = sourceContainer.GetBlockBlobReference(file);\n",
							"    var destBlob = destDirectory.GetBlockBlobReference(file);\n",
							"    await CopyBlob(sourceBlob, destBlob);\n",
							"}\n",
							""
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/download_and_resize_images')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "defsparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "224g",
					"driverCores": 32,
					"executorMemory": "224g",
					"executorCores": 32,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "eb85deed-e183-479b-954c-6da8a647c7c6"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a1a27566-3e3c-42d7-a372-692095cd8521/resourceGroups/SerengetiDataLab/providers/Microsoft.Synapse/workspaces/serengetidatalabqezfaqkj/bigDataPools/defsparkpool",
						"name": "defsparkpool",
						"type": "Spark",
						"endpoint": "https://serengetidatalabqezfaqkj.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/defsparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 32,
						"memory": 224,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%pip install azure-storage-file-datalake\n",
							"%pip install azure-identity"
						],
						"outputs": [],
						"execution_count": 115
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import sys\n",
							"from pyspark.sql import SparkSession\n",
							"\n",
							"sc = SparkSession.builder.getOrCreate()\n",
							"token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
							"\n",
							"vault_name = 'serengetikeyvaultqezfaqk'\n",
							"\n",
							"pool_connection_string = mssparkutils.credentials.getSecret(vault_name, 'DedicatedPool-Jdbc-ConnectionString')\n",
							"adls_connection_string = mssparkutils.credentials.getSecret(vault_name, 'ADLS-ConnectionString')"
						],
						"outputs": [],
						"execution_count": 116
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"train_table_name = \"train\"\n",
							"val_table_name = \"val\"\n",
							"\n",
							"df_train = spark.read \\\n",
							"  .format(\"jdbc\") \\\n",
							"  .option(\"url\", pool_connection_string) \\\n",
							"  .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"  .option(\"dbTable\", train_table_name) \\\n",
							"  .load()\n",
							"\n",
							"df_train = df_train.toPandas()\n",
							"\n",
							"df_val = spark.read \\\n",
							"  .format(\"jdbc\") \\\n",
							"  .option(\"url\", pool_connection_string) \\\n",
							"  .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"  .option(\"dbTable\", val_table_name) \\\n",
							"  .load()\n",
							"\n",
							"df_val = df_val.toPandas()"
						],
						"outputs": [],
						"execution_count": 117
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def imageURL(img_id):\n",
							"    return f\"https://lilablobssc.blob.core.windows.net/snapshotserengeti-unzipped/{img_id}.JPG\"\n",
							"\n",
							"df_train[\"image_link\"] = df_train[\"image_id\"].apply(imageURL)\n",
							"df_val[\"image_link\"] = df_val[\"image_id\"].apply(imageURL)"
						],
						"outputs": [],
						"execution_count": 118
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.filedatalake import DataLakeServiceClient\n",
							"\n",
							"def initialize_adls_account():\n",
							"    \n",
							"    try:  \n",
							"        global service_client\n",
							"\n",
							"        service_client = DataLakeServiceClient.from_connection_string(adls_connection_string)\n",
							"    \n",
							"    except Exception as e:\n",
							"        print(e)"
						],
						"outputs": [],
						"execution_count": 119
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import urllib.request\n",
							"import io\n",
							"from PIL import Image\n",
							"from azure.identity import DefaultAzureCredential\n",
							"from azure.storage.filedatalake import DataLakeFileClient\n",
							"\n",
							"\n",
							"def resize_image (image_url):\n",
							"    # Download the image from the URL\n",
							"    with urllib.request.urlopen(image_url) as url:\n",
							"        image_data = url.read()\n",
							"    \n",
							"    # Open the downloaded image using PIL\n",
							"    image = Image.open(io.BytesIO(image_data))\n",
							"\n",
							"    # Resize the image to 224x224 pixels\n",
							"    return image.resize((224, 224))\n",
							"\n",
							"\n",
							"def upload_image(image_url, image_name, category, table):\n",
							"    \n",
							"    resized_image = resize_image(image_url)\n",
							"\n",
							"    # Save the resized image to Azure Data Lake Storage\n",
							"    credential = DefaultAzureCredential()\n",
							"    file_system_name = 'snapshot-serengeti'\n",
							"    directory = 'images'\n",
							"    file_name = f'{image_name}.JPG'\n",
							"\n",
							"    try:\n",
							"\n",
							"        file_system_client = service_client.get_file_system_client(file_system_name)\n",
							"        directory_client = file_system_client.get_directory_client(directory)   \n",
							"        file_client = directory_client.create_file(f'{table}/{category}/{file_name}')\n",
							"\n",
							"        with io.BytesIO() as output:\n",
							"            resized_image.save(output, format='JPEG')\n",
							"            output.seek(0)\n",
							"            file_client.upload_data(data=output, overwrite=True)\n",
							"\n",
							"\n",
							"    except Exception as e:\n",
							"      print(e)\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 120
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"initialize_adls_account()"
						],
						"outputs": [],
						"execution_count": 121
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def upload_image_parallel(args):\n",
							"    image_url, image_name, category, table = args\n",
							"    upload_image(image_url, image_name, category, table)"
						],
						"outputs": [],
						"execution_count": 122
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"for item in df_train.iloc:\n",
							"    upload_image(item.image_link, item.image_id.split('/').pop(), item.category_name, 'train')"
						],
						"outputs": [],
						"execution_count": 123
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"for item in df_val.iloc:\n",
							"    upload_image(item.image_link, item.image_id.split('/').pop(), item.category_name, 'val')"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/save_json_data_to_sql')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "defsparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "224g",
					"driverCores": 32,
					"executorMemory": "224g",
					"executorCores": 32,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c215d259-bdac-4f91-b813-aa52332fe137"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "csharp"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a1a27566-3e3c-42d7-a372-692095cd8521/resourceGroups/SerengetiDataLab/providers/Microsoft.Synapse/workspaces/serengetidatalabqezfaqkj/bigDataPools/defsparkpool",
						"name": "defsparkpool",
						"type": "Spark",
						"endpoint": "https://serengetidatalabqezfaqkj.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/defsparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 32,
						"memory": 224
					},
					"sessionKeepAliveTimeout": 120
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"#r \"nuget: Microsoft.Azure.Storage.Common\"\n",
							"#r \"nuget: Microsoft.Azure.Storage.Blob\"\n",
							"#r \"nuget: Microsoft.Azure.Storage.File\"\n",
							"#r \"nuget: Newtonsoft.Json\"\n",
							"#r \"nuget: System.Data.SqlClient\""
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"using Microsoft.Azure.Storage;\n",
							"using Microsoft.Azure.Storage.Blob;\n",
							"using System.IO;\n",
							"using Newtonsoft.Json;\n",
							"using System.Data.SqlClient;\n",
							"using System.Data;\n",
							"using System.Threading;\n",
							"using Microsoft.Spark.Extensions.Azure.Synapse.Analytics.Notebook.MSSparkUtils;\n",
							"\n",
							"using System.Collections.Concurrent;"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"public class SerengetiData\n",
							"{\n",
							"    [JsonProperty(\"info\")]\n",
							"    public Info Info { get; set; }\n",
							"\n",
							"    [JsonProperty(\"categories\")]\n",
							"    public List<Category> Categories { get; set; }\n",
							"\n",
							"    [JsonProperty(\"images\")]\n",
							"    public List<Image> Images { get; set; }\n",
							"\n",
							"    [JsonProperty(\"annotations\")]\n",
							"    public List<Annotation> Annotations { get; set; }\n",
							"\n",
							"    public SerengetiData()\n",
							"    {\n",
							"        Info = new Info();\n",
							"        Categories = new List<Category>();\n",
							"        Images = new List<Image>();\n",
							"        Annotations = new List<Annotation>();\n",
							"    }\n",
							"}\n",
							"\n",
							"public class Annotation\n",
							"{\n",
							"    [JsonProperty(\"sequence_level_annotation\")]\n",
							"    public bool SequenceLevelAnnotation { get; set; }\n",
							"\n",
							"    [JsonProperty(\"id\")]\n",
							"    public string Id { get; set; }\n",
							"\n",
							"    [JsonProperty(\"category_id\")]\n",
							"    public long CategoryId { get; set; }\n",
							"\n",
							"    [JsonProperty(\"seq_id\")]\n",
							"    public string SeqId { get; set; }\n",
							"\n",
							"    [JsonProperty(\"season\")]\n",
							"    public string Season { get; set; }\n",
							"\n",
							"    [JsonProperty(\"datetime\")]\n",
							"    public DateTimeOffset Datetime { get; set; }\n",
							"\n",
							"    [JsonProperty(\"subject_id\")]\n",
							"    public string SubjectId { get; set; }\n",
							"\n",
							"    [JsonProperty(\"count\")]\n",
							"    public object Count { get; set; }\n",
							"\n",
							"    [JsonProperty(\"standing\")]\n",
							"    public object Standing { get; set; }\n",
							"\n",
							"    [JsonProperty(\"resting\")]\n",
							"    public object Resting { get; set; }\n",
							"\n",
							"    [JsonProperty(\"moving\")]\n",
							"    public object Moving { get; set; }\n",
							"\n",
							"    [JsonProperty(\"interacting\")]\n",
							"    public object Interacting { get; set; }\n",
							"\n",
							"    [JsonProperty(\"young_present\")]\n",
							"    public object YoungPresent { get; set; }\n",
							"\n",
							"    [JsonProperty(\"image_id\")]\n",
							"    public string ImageId { get; set; }\n",
							"\n",
							"    [JsonProperty(\"location\")]\n",
							"    public string Location { get; set; }\n",
							"}\n",
							"\n",
							"public class Category\n",
							"{\n",
							"    [JsonProperty(\"id\")]\n",
							"    public long Id { get; set; }\n",
							"\n",
							"    [JsonProperty(\"name\")]\n",
							"    public string Name { get; set; }\n",
							"}\n",
							"\n",
							"public class Image\n",
							"{\n",
							"    [JsonProperty(\"id\")]\n",
							"    public string Id { get; set; }\n",
							"\n",
							"    [JsonProperty(\"file_name\")]\n",
							"    public string FileName { get; set; }\n",
							"\n",
							"    [JsonProperty(\"frame_num\")]\n",
							"    public long FrameNum { get; set; }\n",
							"\n",
							"    [JsonProperty(\"seq_id\")]\n",
							"    public string SeqId { get; set; }\n",
							"\n",
							"    [JsonProperty(\"width\")]\n",
							"    public long Width { get; set; }\n",
							"\n",
							"    [JsonProperty(\"height\")]\n",
							"    public long Height { get; set; }\n",
							"\n",
							"    [JsonProperty(\"corrupt\")]\n",
							"    public bool Corrupt { get; set; }\n",
							"\n",
							"    [JsonProperty(\"location\")]\n",
							"    public string Location { get; set; }\n",
							"\n",
							"    [JsonProperty(\"seq_num_frames\")]\n",
							"    public long SeqNumFrames { get; set; }\n",
							"\n",
							"    [JsonProperty(\"datetime\")]\n",
							"    public DateTimeOffset Datetime { get; set; }\n",
							"}\n",
							"\n",
							"public class Info\n",
							"{\n",
							"    [JsonProperty(\"version\")]\n",
							"    public string Version { get; set; }\n",
							"\n",
							"    [JsonProperty(\"description\")]\n",
							"    public string Description { get; set; }\n",
							"\n",
							"    [JsonProperty(\"date_created\")]\n",
							"    public long DateCreated { get; set; }\n",
							"\n",
							"    [JsonProperty(\"contributor\")]\n",
							"    public string Contributor { get; set; }\n",
							"}"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"CloudBlobContainer blobContainer;\n",
							"CloudBlobDirectory blobDirectory;\n",
							"\n",
							"var vaultName = \"serengetikeyvaultqezfaqk\";\n",
							"\n",
							"string storageConnectionString = Credentials.GetSecret(vaultName,\"ADLS-ConnectionString\");\n",
							"var dbConnectionString = Credentials.GetSecret(vaultName,\"DedicatedPool-ConnectionString\");\n",
							"\n",
							"private void InitStorageAndDb()\n",
							"{\n",
							"    // Create a FileEndpoint for the destination ADLS\n",
							"    CloudStorageAccount storageAccount = CloudStorageAccount.Parse(storageConnectionString);\n",
							"\n",
							"    var blobClient= storageAccount.CreateCloudBlobClient();\n",
							"    blobContainer =  blobClient.GetContainerReference(\"snapshot-serengeti\");\n",
							"    blobDirectory = blobContainer.GetDirectoryReference(\"metadata\");\n",
							"}"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"private async Task<T> ReadJsonFileAsync<T> (CloudBlockBlob jsonBlob)\n",
							"{\n",
							"    using (var memoryStream = new MemoryStream())\n",
							"    {\n",
							"        // Download the JSON file to a memory stream\n",
							"        await jsonBlob.DownloadToStreamAsync(memoryStream);\n",
							"\n",
							"        // Reset the memory stream position\n",
							"        memoryStream.Position = 0;\n",
							"\n",
							"        // Use a JsonTextReader to read the JSON file in chunks\n",
							"        using (var jsonTextReader = new JsonTextReader(new StreamReader(memoryStream)) { CloseInput = false })\n",
							"        {\n",
							"            // Use a JsonSerializer to deserialize the JSON file\n",
							"            var jsonSerializer = new JsonSerializer();\n",
							"\n",
							"            // Read the JSON file in chunks and deserialize it\n",
							"            return jsonSerializer.Deserialize<T>(jsonTextReader);\n",
							"        }\n",
							"    }\n",
							"}\n",
							""
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"private async Task CreateTablesAsync()\n",
							"{\n",
							"    var commands = new Dictionary<string, string>()\n",
							"    {\n",
							"        {\"images\", \"CREATE TABLE images (id VARCHAR(255), file_name VARCHAR(255), frame_num INT, seq_id VARCHAR(255), width INT, height INT, corrupt BIT, location VARCHAR(255), seq_num_frames INT, datetime DATETIME);\"},\n",
							"        {\"categories\", \"CREATE TABLE categories (id INT, name VARCHAR(255));\"},\n",
							"        {\"annotations\", \"CREATE TABLE annotations ( id VARCHAR(255) NOT NULL, category_id INT NOT NULL, seq_id VARCHAR(255) NOT NULL, season VARCHAR(255) NOT NULL, datetime DATETIME NOT NULL, image_id VARCHAR(255) NOT NULL, location VARCHAR(255) NOT NULL );\"},\n",
							"        {\"train\", \"CREATE TABLE train ( image_id VARCHAR(255), category_name VARCHAR(255));\"},\n",
							"        {\"val\", \"CREATE TABLE val ( image_id VARCHAR(255), category_name VARCHAR(255));\"}\n",
							"    };\n",
							"\n",
							"    using(var conn = new SqlConnection(dbConnectionString))\n",
							"    {\n",
							"        await conn.OpenAsync();\n",
							"        foreach(var command in commands)\n",
							"        {\n",
							"            using(SqlCommand sqlCmd =new SqlCommand(command.Value, conn))\n",
							"            {\n",
							"                try\n",
							"                {\n",
							"                    await sqlCmd.ExecuteNonQueryAsync();\n",
							"                    Console.WriteLine($\"Table {command.Key} created successfully.\");\n",
							"                }\n",
							"                catch(Exception ex)\n",
							"                {\n",
							"                    Console.WriteLine($\"Error creating table {command.Key}: \" + ex.Message);\n",
							"                }\n",
							"            }   \n",
							"        }\n",
							"    }\n",
							"}"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"private async Task BulkInsertImages(List<Image> images)\n",
							"{\n",
							"    using(var conn = new SqlConnection(dbConnectionString))\n",
							"    {\n",
							"        await conn.OpenAsync();\n",
							"        using(var bulkCpy = new SqlBulkCopy(conn))\n",
							"        {\n",
							"            bulkCpy.DestinationTableName=\"images\";\n",
							"\n",
							"            var dataTable = new DataTable();\n",
							"            dataTable.Columns.Add(\"id\", typeof(string));\n",
							"            dataTable.Columns.Add(\"file_name\", typeof(string));\n",
							"            dataTable.Columns.Add(\"frame_num\", typeof(long));\n",
							"            dataTable.Columns.Add(\"seq_id\", typeof(string));\n",
							"            dataTable.Columns.Add(\"width\", typeof(long));\n",
							"            dataTable.Columns.Add(\"height\", typeof(long));\n",
							"            dataTable.Columns.Add(\"corrupt\", typeof(bool));\n",
							"            dataTable.Columns.Add(\"location\", typeof(string));\n",
							"            dataTable.Columns.Add(\"seq_num_frames\", typeof(long));\n",
							"            dataTable.Columns.Add(\"datetime\", typeof(DateTime));\n",
							"\n",
							"            foreach (var image in images)\n",
							"            {\n",
							"                var row = dataTable.NewRow();\n",
							"                row[\"id\"] = image.Id;\n",
							"                row[\"file_name\"] = image.FileName;\n",
							"                row[\"frame_num\"] = image.FrameNum;\n",
							"                row[\"seq_id\"] = image.SeqId;\n",
							"                row[\"width\"] = image.Width;\n",
							"                row[\"height\"] = image.Height;\n",
							"                row[\"corrupt\"] = image.Corrupt;\n",
							"                row[\"location\"] = image.Location;\n",
							"                row[\"seq_num_frames\"] = image.SeqNumFrames;\n",
							"                row[\"datetime\"] = image.Datetime.DateTime;\n",
							"\n",
							"                dataTable.Rows.Add(row);\n",
							"            }\n",
							"\n",
							"            await Task.Run(() => bulkCpy.WriteToServer(dataTable));\n",
							"        }\n",
							"    }\n",
							"}\n",
							""
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"private async Task BulkInsertCategories(List<Category> categories)\n",
							"{\n",
							"    using(var conn = new SqlConnection(dbConnectionString))\n",
							"    {\n",
							"        await conn.OpenAsync();\n",
							"        using (var bulkCpy = new SqlBulkCopy(conn))\n",
							"        {\n",
							"            bulkCpy.DestinationTableName = \"categories\";\n",
							"\n",
							"            var dataTable = new DataTable();\n",
							"            dataTable.Columns.Add(\"id\", typeof(long));\n",
							"            dataTable.Columns.Add(\"name\", typeof(string));\n",
							"\n",
							"            foreach(var category in categories)\n",
							"            {\n",
							"                var row = dataTable.NewRow();\n",
							"                row[\"id\"] = category.Id;\n",
							"                row[\"name\"] = category.Name;\n",
							"                dataTable.Rows.Add(row);\n",
							"            }\n",
							"\n",
							"            await Task.Run(() => bulkCpy.WriteToServer(dataTable));\n",
							"        }\n",
							"    }\n",
							"}\n",
							""
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"private async Task BulkInsertAnnotations(List<Annotation> annotations)\n",
							"{\n",
							"    using(var conn = new SqlConnection(dbConnectionString))\n",
							"    {\n",
							"        await conn.OpenAsync();\n",
							"        using (var bulkCpy = new SqlBulkCopy(conn))\n",
							"        {\n",
							"            bulkCpy.DestinationTableName = \"annotations\";\n",
							"\n",
							"            var dataTable = new DataTable();\n",
							"            dataTable.Columns.Add(\"id\", typeof(string));\n",
							"            dataTable.Columns.Add(\"category_id\", typeof(long));\n",
							"            dataTable.Columns.Add(\"seq_id\", typeof(string));\n",
							"            dataTable.Columns.Add(\"season\", typeof(string));\n",
							"            dataTable.Columns.Add(\"datetime\", typeof(DateTime));\n",
							"            dataTable.Columns.Add(\"image_id\", typeof(string));\n",
							"            dataTable.Columns.Add(\"location\", typeof(string));\n",
							"\n",
							"            foreach (var annotation in annotations)\n",
							"            {\n",
							"                var row = dataTable.NewRow();\n",
							"                row[\"id\"] = annotation.Id;\n",
							"                row[\"category_id\"] = annotation.CategoryId;\n",
							"                row[\"seq_id\"] = annotation.SeqId;\n",
							"                row[\"season\"] = annotation.Season;\n",
							"                row[\"datetime\"] = annotation.Datetime.LocalDateTime;\n",
							"                row[\"image_id\"] = annotation.ImageId;\n",
							"                row[\"location\"] = annotation.Location;\n",
							"                dataTable.Rows.Add(row);\n",
							"            }\n",
							"\n",
							"            await bulkCpy.WriteToServerAsync(dataTable);\n",
							"        }\n",
							"    }\n",
							"}\n",
							""
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"private async Task InsertSplitImages(string table, List<string> locations)\n",
							"{\n",
							"    using(var conn = new SqlConnection(dbConnectionString))\n",
							"    {\n",
							"        await conn.OpenAsync();\n",
							"        using (SqlCommand command = new SqlCommand($\"INSERT INTO {table} (image_id, category_name) SELECT annotations.image_id, categories.name FROM annotations JOIN categories ON annotations.category_id = categories.id WHERE annotations.location IN ({string.Join(\",\", locations.Select(x => $\"'{x}'\"))}) AND annotations.category_id > 1;\", conn)\n",
							")\n",
							"        {\n",
							"            command.ExecuteNonQuery();\n",
							"        }\n",
							"    }\n",
							"}"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"InitStorageAndDb();\n",
							"await CreateTablesAsync();\n",
							"\n",
							"//season files\n",
							"var seasonFiles = new List<string>()\n",
							"{\n",
							"    \"SnapshotSerengetiS01.json\",\n",
							"    \"SnapshotSerengetiS02.json\",\n",
							"    \"SnapshotSerengetiS03.json\",\n",
							"    \"SnapshotSerengetiS04.json\",\n",
							"    \"SnapshotSerengetiS05.json\",\n",
							"    \"SnapshotSerengetiS06.json\",\n",
							"    \"SnapshotSerengetiS07.json\",\n",
							"    \"SnapshotSerengetiS08.json\",\n",
							"    \"SnapshotSerengetiS09.json\",\n",
							"    \"SnapshotSerengetiS10.json\",\n",
							"    \"SnapshotSerengetiS11.json\"\n",
							"};"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"var firstFile = seasonFiles.First();\n",
							"var firstBlob = blobDirectory.GetBlockBlobReference(firstFile);\n",
							"var firstSerengetiData = await ReadJsonFileAsync<SerengetiData>(firstBlob);\n",
							"\n",
							"await BulkInsertCategories(firstSerengetiData.Categories);\n",
							"await BulkInsertImages(firstSerengetiData.Images);\n",
							"await BulkInsertAnnotations(firstSerengetiData.Annotations);\n",
							"\n",
							"Console.WriteLine($\"Completed processing {firstFile} \");\n",
							"\n",
							"\n",
							"var tasks = new List<Task>();\n",
							"\n",
							"Parallel.For(1, seasonFiles.Count, i =>\n",
							"{\n",
							"    tasks.Add(Task.Run(async () =>\n",
							"    {\n",
							"        var file = seasonFiles[i];\n",
							"        var blob = blobDirectory.GetBlockBlobReference(file);\n",
							"        var serengetiData = await ReadJsonFileAsync<SerengetiData>(blob);\n",
							"        await BulkInsertImages(serengetiData.Images);\n",
							"        await BulkInsertAnnotations(serengetiData.Annotations);\n",
							"\n",
							"        Console.WriteLine($\"Completed processing {file} \");\n",
							"    }));\n",
							"});\n",
							"\n",
							"await Task.WhenAll(tasks);\n",
							""
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"var blob = blobDirectory.GetBlockBlobReference(\"SnapshotSerengetiSplits_v0.json\");\n",
							"var splitData = await ReadJsonFileAsync<dynamic>(blob);\n",
							"\n",
							"await InsertSplitImages(\"train\", splitData.splits.train.ToObject<List<string>>());\n",
							"await InsertSplitImages(\"val\", splitData.splits.val.ToObject<List<string>>());"
						],
						"outputs": [],
						"execution_count": 26
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/training-notebook')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "UseThiosMlPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "30g",
					"driverCores": 4,
					"executorMemory": "60g",
					"executorCores": 12,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4cf50a62-7857-483a-8dce-cdca56d91136"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/a1a27566-3e3c-42d7-a372-692095cd8521/resourceGroups/serengeti/providers/Microsoft.Synapse/workspaces/serengetidatalabb37kfc5h/bigDataPools/UseThiosMlPool",
						"name": "UseThiosMlPool",
						"type": "Spark",
						"endpoint": "https://serengetidatalabb37kfc5h.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/UseThiosMlPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 16,
						"memory": 110
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install keras"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install keras-applications"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install tensorflow"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"\r\n",
							"spark = SparkSession \\\r\n",
							"    .builder \\\r\n",
							"    .appName(\"Create Spark Dataframe\") \\\r\n",
							"    .getOrCreate()"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.conf.set(\"\", \"\")\r\n",
							"\r\n",
							"Database = \"defdedicated\"\r\n",
							"Server = \"serengetidatalabb37kfc5h.sql.azuresynapse.net\"\r\n",
							"User = \"sqladminuser@serengetidatalabb37kfc5h\"\r\n",
							"# Pass = sqladmin00!\r\n",
							"JdbcPort =  \"1433\"\r\n",
							"Pass = \"5R&SC4phRemt$d6S\"\r\n",
							"JdbcExtraOptions = \"encrypt=true;trustServerCertificate=true;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\"\r\n",
							"\r\n",
							"sqlUrl = f\"jdbc:sqlserver://{Server}:{JdbcPort};database={Database};user={User};password={Pass};${JdbcExtraOptions}\"\r\n",
							"\r\n",
							"tableName = \"train\"\r\n",
							"\r\n",
							"tempDir = \"abfss://snapshot-serengeti@serengeti.dfs.core.windows.net/\"\r\n",
							"\r\n",
							"df = spark.read \\\r\n",
							"  .format(\"jdbc\") \\\r\n",
							"  .option(\"url\", sqlUrl) \\\r\n",
							"  .option(\"tempDir\", tempDir) \\\r\n",
							"  .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\r\n",
							"  .option(\"dbTable\", tableName) \\\r\n",
							"  .load()\r\n",
							"\r\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"tableName2 = \"val\"\r\n",
							"\r\n",
							"df_val = spark.read \\\r\n",
							"  .format(\"jdbc\") \\\r\n",
							"  .option(\"url\", sqlUrl) \\\r\n",
							"  .option(\"tempDir\", tempDir) \\\r\n",
							"  .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\r\n",
							"  .option(\"dbTable\", tableName2) \\\r\n",
							"  .load()\r\n",
							"\r\n",
							"df_val.show()"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install keras --upgrade"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import numpy as np\r\n",
							"import keras\r\n",
							"from keras.applications import ResNet50\r\n",
							"from keras.preprocessing.image import ImageDataGenerator\r\n",
							"from keras.preprocessing import image\r\n",
							"#from keras.applications.resnet50 import preprocess_input\r\n",
							"from keras.models import Model\r\n",
							"from keras.layers import Dense, GlobalAveragePooling2D\r\n",
							"from keras.optimizers import Adam\r\n",
							"import pandas as pd \r\n",
							"from PIL import Image\r\n",
							"import requests\r\n",
							""
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas as pd \r\n",
							"df = df.toPandas()"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def imageURL(img_id):\r\n",
							"    return f\"https://lilablobssc.blob.core.windows.net/snapshotserengeti-unzipped/{img_id}.JPG\"\r\n",
							"\r\n",
							"df_train[\"image_link\"] = df_train[\"image_id\"].apply(imageURL)\r\n",
							"df_val[\"image_link\"] = df_val[\"image_id\"].apply(imageURL)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import multiprocessing as mp\r\n",
							"\r\n",
							"def load_image(url, queue):\r\n",
							"    try:\r\n",
							"        response = requests.get(url)\r\n",
							"        img = Image.open(BytesIO(response.content))\r\n",
							"        img = img.resize((224, 224), Image.ANTIALIAS)\r\n",
							"        image = np.array(img)\r\n",
							"        queue.put(image)\r\n",
							"    except:\r\n",
							"        queue.put(None)\r\n",
							"        \r\n",
							"def load_images(image_urls):\r\n",
							"    manager = mp.Manager()\r\n",
							"    queue = manager.Queue()\r\n",
							"    processes = []\r\n",
							"    \r\n",
							"    for url in image_urls:\r\n",
							"        process = mp.Process(target=load_image, args=(url, queue))\r\n",
							"        process.start()\r\n",
							"        processes.append(process)\r\n",
							"        \r\n",
							"    images = []\r\n",
							"    success_count = 0\r\n",
							"    failure_count = 0\r\n",
							"    \r\n",
							"    for _ in range(len(image_urls)):\r\n",
							"        image = queue.get()\r\n",
							"        if image is not None:\r\n",
							"            images.append(image)\r\n",
							"            success_count += 1\r\n",
							"        else:\r\n",
							"            failure_count += 1\r\n",
							"            \r\n",
							"    for process in processes:\r\n",
							"        process.join()\r\n",
							"        \r\n",
							"    train_data = np.stack(images)\r\n",
							"    \r\n",
							"    print(\"Successful loads:\", success_count)\r\n",
							"    print(\"Failed loads:\", failure_count)\r\n",
							"    \r\n",
							"    return train_data\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sub_set = df_train.iloc[:4][\"image_link\"]\r\n",
							"sub_set"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"train_data = load_images(sub_set)\r\n",
							"# val_data = load_images(df_val.iloc[0][\"image_link\"])\r\n",
							"     "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"labels_map = {\r\n",
							"    'empty': 0,\r\n",
							"    'human': 1,\r\n",
							"    'gazellegrants': 2,\r\n",
							"    'reedbuck': 3,\r\n",
							"    'dikdik': 4,\r\n",
							"    'zebra': 5,\r\n",
							"    'porcupine': 6,\r\n",
							"    'gazellethomsons': 7,\r\n",
							"    'hyenaspotted': 8,\r\n",
							"    'warthog': 9,\r\n",
							"    'impala': 10,\r\n",
							"    'elephant': 11,\r\n",
							"    'aardvark': 12,\r\n",
							"    'giraffe': 13,\r\n",
							"    'mongoose': 14,\r\n",
							"    'buffalo': 15,\r\n",
							"    'hartebeest': 16,\r\n",
							"    'guineafowl': 17,\r\n",
							"    'wildebeest': 18,\r\n",
							"    'leopard': 19,\r\n",
							"    'ostrich': 20,\r\n",
							"    'lionfemale': 21,\r\n",
							"    'koribustard': 22,\r\n",
							"    'otherbird': 23,\r\n",
							"    'cheetah': 24,\r\n",
							"    'honeybadger': 25,\r\n",
							"    'bushbuck': 26,\r\n",
							"    'jackal': 27,\r\n",
							"    'aardwolf': 28,\r\n",
							"    'hippopotamus': 29,\r\n",
							"    'hyenastriped': 30,\r\n",
							"    'hare': 31,\r\n",
							"    'baboon': 32,\r\n",
							"    'monkeyvervet': 33,\r\n",
							"    'batearedfox': 34,\r\n",
							"    'waterbuck': 35,\r\n",
							"    'secretarybird': 36,\r\n",
							"    'topi': 37,\r\n",
							"    'serval': 38,\r\n",
							"    'lionmale': 39,\r\n",
							"    'eland': 40,\r\n",
							"    'rodents': 41,\r\n",
							"    'wildcat': 42,\r\n",
							"    'civet': 43,\r\n",
							"    'genet': 44,\r\n",
							"    'zorilla': 45,\r\n",
							"    'caracal': 46,\r\n",
							"    'rhinoceros': 47,\r\n",
							"    'reptiles': 48,\r\n",
							"    'insectspider': 49,\r\n",
							"    'duiker': 50,\r\n",
							"    'cattle': 51,\r\n",
							"    'vulture': 52,\r\n",
							"    'steenbok': 53,\r\n",
							"    'bat': 54,\r\n",
							"    'fire': 55,\r\n",
							"    'hyenabrown': 56,\r\n",
							"    'wilddog': 57,\r\n",
							"    'kudu': 58,\r\n",
							"    'pangolin': 59,\r\n",
							"    'lioncub': 60\r\n",
							"}\r\n",
							"\r\n",
							"image_labels = ['empty', 'human', 'gazellegrants', 'reedbuck', 'dikdik', 'zebra', 'porcupine', 'gazellethomsons', 'hyenaspotted', 'warthog', 'impala', 'elephant', 'aardvark', 'giraffe', 'mongoose', 'buffalo', 'hartebeest', 'guineafowl', 'wildebeest', 'leopard', 'ostrich', 'lionfemale', 'koribustard', 'otherbird', 'cheetah', 'honeybadger', 'bushbuck', 'jackal', 'aardwolf', 'hippopotamus', 'hyenastriped', 'hare', 'baboon', 'monkeyvervet', 'batearedfox', 'waterbuck', 'secretarybird', 'topi', 'serval', 'lionmale', 'eland', 'rodents', 'wildcat', 'civet', 'genet', 'zorilla', 'caracal', 'rhinoceros', 'reptiles', 'insectspider', 'duiker', 'cattle', 'vulture', 'steenbok', 'bat', 'fire', 'hyenabrown', 'wilddog', 'kudu', 'pangolin', 'lioncub']\r\n",
							"\r\n",
							"\r\n",
							"     "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import numpy as np\r\n",
							"\r\n",
							"def one_hot_encode_labels(image_labels, labels_map):\r\n",
							"    # Convert the label names to integers using the labels_map dictionary\r\n",
							"    label_ints = [labels_map[label] for label in image_labels]\r\n",
							"    # Perform one-hot encoding using np.eye\r\n",
							"    train_labels = np.eye(len(labels_map))[label_ints]\r\n",
							"    return train_labels\r\n",
							"\r\n",
							"train_labels = one_hot_encode_labels(image_labels, labels_map)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import keras\r\n",
							"from keras.applications import ResNet50\r\n",
							"from keras.preprocessing import image\r\n",
							"from keras.layers import Dense, GlobalAveragePooling2D\r\n",
							"from keras.optimizers import SGD\r\n",
							"from keras.models import Model\r\n",
							"from keras.preprocessing.image import ImageDataGenerator\r\n",
							"\r\n",
							"num_classes = 59\r\n",
							"# Load the ResNet50 model pre-trained on ImageNet\r\n",
							"resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))\r\n",
							"\r\n",
							"# Freeze all layers of the pre-trained model\r\n",
							"for layer in resnet_model.layers:\r\n",
							"    layer.trainable = False\r\n",
							"\r\n",
							"# Add new trainable layers on top of the frozen layers\r\n",
							"x = resnet_model.output\r\n",
							"x = GlobalAveragePooling2D()(x)\r\n",
							"x = Dense(1024, activation='relu')(x)\r\n",
							"x = Dense(512, activation='relu')(x)\r\n",
							"predictions = Dense(num_classes, activation='softmax')(x)\r\n",
							"\r\n",
							"# Compile the model\r\n",
							"model = Model(inputs=resnet_model.input, outputs=predictions)\r\n",
							"model.compile(optimizer=SGD(lr=0.001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
							"\r\n",
							"# Create ImageDataGenerator objects for the training and validation sets\r\n",
							"train_datagen = ImageDataGenerator(rescale=1./255)\r\n",
							"val_datagen = ImageDataGenerator(rescale=1./255)\r\n",
							"            \r\n",
							"            \r\n",
							"# Use the flow_from_dataframe method to load the data\r\n",
							"train_generator = train_datagen.flow_from_dataframe(\r\n",
							"    dataframe=df,\r\n",
							"    x_col='image_link',\r\n",
							"    y_col='name',\r\n",
							"    directory=None,\r\n",
							"    target_size=(224,224),\r\n",
							"    batch_size=32,\r\n",
							"    class_mode='categorical')\r\n",
							"\r\n",
							"val_generator = val_datagen.flow_from_dataframe(\r\n",
							"    dataframe=df_val,\r\n",
							"    x_col='image_link',\r\n",
							"    y_col='name',\r\n",
							"    directory=None,\r\n",
							"    target_size=(224,224),\r\n",
							"    batch_size=32,\r\n",
							"    class_mode='categorical')\r\n",
							"\r\n",
							"# Compile the model\r\n",
							"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
							"\r\n",
							"# Train the model\r\n",
							"model.fit(train_generator, epochs=10, steps_per_epoch=100)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/defdedicated')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westeurope"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/defsparkpool')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "XLarge",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.2",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westeurope"
		}
	]
}